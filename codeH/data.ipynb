{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as ps\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset\n",
    "# This is for the progress bar.\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102_D_94.png .ipynb_checkpoints 141\n",
      "244 244\n"
     ]
    }
   ],
   "source": [
    "#文件夹位置加载\n",
    "train_folder_path = '../data/FAZ/Domain1/train/imgs'\n",
    "mask_folder_path = '../data/FAZ/Domain1/train/mask'\n",
    "test_folder_path='../data/FAZ/Domain1/test/imgs'\n",
    "train_files = os.listdir(train_folder_path)\n",
    "mask_files= os.listdir(mask_folder_path)\n",
    "for i in range(244):\n",
    "    if(mask_files[i]==train_files[i]):\n",
    "        pass\n",
    "    else:\n",
    "        print(mask_files[i],train_files[i],i)\n",
    "        del train_files[i]\n",
    "        break\n",
    "# test_files = os.listdir(test_folder_path)\n",
    "print(len(train_files),len(mask_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义数据集\n",
    "\n",
    "# 转化为tensor\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train_files = None,test_files = None):\n",
    "        super(MyDataset).__init__()\n",
    "        self.train_files =train_files\n",
    "        if train_files != None:\n",
    "            self.train_files = train_files\n",
    "        self.test_files =test_files\n",
    "        if test_files != None:\n",
    "            self.test_files = test_files\n",
    "        print(f\"One sample\",self.train_files[0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.train_files)\n",
    "\n",
    "    # cv读取图像信息\n",
    "    def __getitem__(self,idx):\n",
    "        \n",
    "        fname = self.train_files[idx]\n",
    "        im = cv2.imread(os.path.join(train_folder_path,fname))\n",
    "        #im = self.data[idx]\n",
    "        mname = self.test_files[idx]\n",
    "        segemnt_im = cv2.imread(os.path.join(mask_folder_path,mname))\n",
    "        return transform(im),transform(segemnt_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "#定义网络\n",
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "class Conv_Block(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super(Conv_Block, self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(in_channel,out_channel,3,1,1,padding_mode='reflect',bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, 1, 1, padding_mode='reflect', bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "\n",
    "# 下采样\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self,channel):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.layer=nn.Sequential(\n",
    "            nn.Conv2d(channel,channel,3,2,1,padding_mode='reflect',bias=False),\n",
    "            nn.BatchNorm2d(channel),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "\n",
    "# 上采样\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self,channel):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.layer=nn.Conv2d(channel,channel//2,1,1)\n",
    "    def forward(self,x,feature_map):\n",
    "        up=F.interpolate(x,scale_factor=2,mode='nearest')\n",
    "        out=self.layer(up)\n",
    "        return torch.cat((out,feature_map),dim=1)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.c1=Conv_Block(3,64)\n",
    "        self.d1=DownSample(64)\n",
    "        self.c2=Conv_Block(64,128)\n",
    "        self.d2=DownSample(128)\n",
    "        self.c3=Conv_Block(128,256)\n",
    "        self.d3=DownSample(256)\n",
    "        self.c4=Conv_Block(256,512)\n",
    "        self.d4=DownSample(512)\n",
    "        self.c5=Conv_Block(512,1024)\n",
    "        self.u1=UpSample(1024)\n",
    "        self.c6=Conv_Block(1024,512)\n",
    "        self.u2 = UpSample(512)\n",
    "        self.c7 = Conv_Block(512, 256)\n",
    "        self.u3 = UpSample(256)\n",
    "        self.c8 = Conv_Block(256, 128)\n",
    "        self.u4 = UpSample(128)\n",
    "        self.c9 = Conv_Block(128, 64)\n",
    "        self.out=nn.Conv2d(64,3,3,1,1)\n",
    "        self.Th=nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        R1=self.c1(x)\n",
    "        R2=self.c2(self.d1(R1))\n",
    "        R3 = self.c3(self.d2(R2))\n",
    "        R4 = self.c4(self.d3(R3))\n",
    "        R5 = self.c5(self.d4(R4))\n",
    "        O1=self.c6(self.u1(R5,R4))\n",
    "        O2 = self.c7(self.u2(O1, R3))\n",
    "        O3 = self.c8(self.u3(O2, R2))\n",
    "        O4 = self.c9(self.u4(O3, R1))\n",
    "\n",
    "        return self.Th(self.out(O4))\n",
    "\n",
    "x=torch.randn(2,3,256,256)\n",
    "net=UNet()\n",
    "print(net(x).shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One sample 057_N_60.png\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset(train_files=train_files,test_files=mask_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loss 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss(y_pred, y_real):\n",
    "    eps = 1e-8\n",
    "    y_pred = torch.sigmoid(y_pred)\n",
    "    \n",
    "    num = torch.sum(2*y_real*y_pred)\n",
    "    den = torch.sum(y_real + y_pred)\n",
    "    \n",
    "    return 1-1/(256*256)*(num+eps)/(den+eps)\n",
    "    \n",
    "def focal_loss(y_pred, y_real, eps = 1e-8, gamma = 2):\n",
    "    #y_pred =  # hint: torch.clamp\n",
    "    L = (y_pred.clamp(min=0) - y_pred*y_real + torch.log(1 + torch.exp(-torch.abs(y_pred)))).mean()\n",
    "    focal_loss = 1*(1-torch.exp(-L))**gamma * L\n",
    "    return focal_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not successful load weight\n",
      "0-0-train_loss===>>0.6511476635932922\n",
      "0-10-train_loss===>>0.1412591189146042\n",
      "0-20-train_loss===>>0.07226958125829697\n",
      "0-30-train_loss===>>0.05397291108965874\n",
      "1-0-train_loss===>>0.059064723551273346\n",
      "1-10-train_loss===>>0.06323663890361786\n",
      "1-20-train_loss===>>0.037667166441679\n",
      "1-30-train_loss===>>0.041193507611751556\n",
      "2-0-train_loss===>>0.06496740877628326\n",
      "2-10-train_loss===>>0.05773475021123886\n",
      "2-20-train_loss===>>0.07724453508853912\n",
      "2-30-train_loss===>>0.03169950470328331\n",
      "3-0-train_loss===>>0.05819873511791229\n",
      "3-10-train_loss===>>0.04984600469470024\n",
      "3-20-train_loss===>>0.055752985179424286\n",
      "3-30-train_loss===>>0.04889967665076256\n",
      "4-0-train_loss===>>0.03509267419576645\n",
      "4-10-train_loss===>>0.0689769983291626\n",
      "4-20-train_loss===>>0.031881146132946014\n",
      "4-30-train_loss===>>0.028308790177106857\n",
      "5-0-train_loss===>>0.033066846430301666\n",
      "5-10-train_loss===>>0.03825889155268669\n",
      "5-20-train_loss===>>0.02754279598593712\n",
      "5-30-train_loss===>>0.02492855116724968\n",
      "6-0-train_loss===>>0.04049259424209595\n",
      "6-10-train_loss===>>0.047413259744644165\n",
      "6-20-train_loss===>>0.03409314155578613\n",
      "6-30-train_loss===>>0.029952630400657654\n",
      "7-0-train_loss===>>0.04366239160299301\n",
      "7-10-train_loss===>>0.024729497730731964\n",
      "7-20-train_loss===>>0.03339855372905731\n",
      "7-30-train_loss===>>0.026607971638441086\n",
      "8-0-train_loss===>>0.03535958379507065\n",
      "8-10-train_loss===>>0.030987029895186424\n",
      "8-20-train_loss===>>0.03281936049461365\n",
      "8-30-train_loss===>>0.03602970391511917\n",
      "9-0-train_loss===>>0.03984716162085533\n",
      "9-10-train_loss===>>0.031083479523658752\n",
      "9-20-train_loss===>>0.03502926975488663\n",
      "9-30-train_loss===>>0.028975604102015495\n",
      "10-0-train_loss===>>0.02517608366906643\n",
      "10-10-train_loss===>>0.04380613565444946\n",
      "10-20-train_loss===>>0.031823113560676575\n",
      "10-30-train_loss===>>0.0382024385035038\n",
      "11-0-train_loss===>>0.02735535427927971\n",
      "11-10-train_loss===>>0.03512241691350937\n",
      "11-20-train_loss===>>0.05691352114081383\n",
      "11-30-train_loss===>>0.024403009563684464\n",
      "12-0-train_loss===>>0.03255306929349899\n",
      "12-10-train_loss===>>0.030949806794524193\n",
      "12-20-train_loss===>>0.037677086889743805\n",
      "12-30-train_loss===>>0.03121369145810604\n",
      "13-0-train_loss===>>0.03402445465326309\n",
      "13-10-train_loss===>>0.0315713994204998\n",
      "13-20-train_loss===>>0.02814020775258541\n",
      "13-30-train_loss===>>0.03871000558137894\n",
      "14-0-train_loss===>>0.05135636776685715\n",
      "14-10-train_loss===>>0.03444855660200119\n",
      "14-20-train_loss===>>0.04657498002052307\n",
      "14-30-train_loss===>>0.041072241961956024\n",
      "15-0-train_loss===>>0.03230031579732895\n",
      "15-10-train_loss===>>0.03178840130567551\n",
      "15-20-train_loss===>>0.02335277758538723\n",
      "15-30-train_loss===>>0.03044196590781212\n",
      "16-0-train_loss===>>0.029417797923088074\n",
      "16-10-train_loss===>>0.04279489070177078\n",
      "16-20-train_loss===>>0.04640297219157219\n",
      "16-30-train_loss===>>0.030474774539470673\n",
      "17-0-train_loss===>>0.03031466156244278\n",
      "17-10-train_loss===>>0.03473789244890213\n",
      "17-20-train_loss===>>0.0327211432158947\n",
      "17-30-train_loss===>>0.03913106769323349\n",
      "18-0-train_loss===>>0.02962568961083889\n",
      "18-10-train_loss===>>0.025737226009368896\n",
      "18-20-train_loss===>>0.045523665845394135\n",
      "18-30-train_loss===>>0.02729453705251217\n",
      "19-0-train_loss===>>0.060010939836502075\n",
      "19-10-train_loss===>>0.02594549022614956\n",
      "19-20-train_loss===>>0.047097183763980865\n",
      "19-30-train_loss===>>0.03451120853424072\n",
      "20-0-train_loss===>>0.03576061129570007\n",
      "20-10-train_loss===>>0.024005036801099777\n",
      "20-20-train_loss===>>0.029022425413131714\n",
      "20-30-train_loss===>>0.030156752094626427\n",
      "21-0-train_loss===>>0.0296770092099905\n",
      "21-10-train_loss===>>0.027759995311498642\n",
      "21-20-train_loss===>>0.03643123060464859\n",
      "21-30-train_loss===>>0.023694556206464767\n",
      "22-0-train_loss===>>0.028933044523000717\n",
      "22-10-train_loss===>>0.045978084206581116\n",
      "22-20-train_loss===>>0.03860175237059593\n",
      "22-30-train_loss===>>0.023887384682893753\n",
      "23-0-train_loss===>>0.03401634842157364\n",
      "23-10-train_loss===>>0.02508874051272869\n",
      "23-20-train_loss===>>0.030425479635596275\n",
      "23-30-train_loss===>>0.02294994704425335\n",
      "24-0-train_loss===>>0.022564154118299484\n",
      "24-10-train_loss===>>0.02258870005607605\n",
      "24-20-train_loss===>>0.026609309017658234\n",
      "24-30-train_loss===>>0.030696582049131393\n",
      "25-0-train_loss===>>0.0507146492600441\n",
      "25-10-train_loss===>>0.02461640164256096\n",
      "25-20-train_loss===>>0.044150423258543015\n",
      "25-30-train_loss===>>0.03969709202647209\n",
      "26-0-train_loss===>>0.02499416470527649\n",
      "26-10-train_loss===>>0.03028048388659954\n",
      "26-20-train_loss===>>0.029127292335033417\n",
      "26-30-train_loss===>>0.028564713895320892\n",
      "27-0-train_loss===>>0.026629816740751266\n",
      "27-10-train_loss===>>0.027813486754894257\n",
      "27-20-train_loss===>>0.026378586888313293\n",
      "27-30-train_loss===>>0.03597510606050491\n",
      "28-0-train_loss===>>0.021478500217199326\n",
      "28-10-train_loss===>>0.028443172574043274\n",
      "28-20-train_loss===>>0.02369680628180504\n",
      "28-30-train_loss===>>0.03125132620334625\n",
      "29-0-train_loss===>>0.026773838326334953\n",
      "29-10-train_loss===>>0.034081146121025085\n",
      "29-20-train_loss===>>0.03072602115571499\n",
      "29-30-train_loss===>>0.02641092613339424\n",
      "30-0-train_loss===>>0.030158227309584618\n",
      "30-10-train_loss===>>0.035638757050037384\n",
      "30-20-train_loss===>>0.024806909263134003\n",
      "30-30-train_loss===>>0.033576346933841705\n",
      "31-0-train_loss===>>0.030854880809783936\n",
      "31-10-train_loss===>>0.025857096537947655\n",
      "31-20-train_loss===>>0.02903285250067711\n",
      "31-30-train_loss===>>0.034508880227804184\n",
      "32-0-train_loss===>>0.031103624030947685\n",
      "32-10-train_loss===>>0.025002047419548035\n",
      "32-20-train_loss===>>0.026019379496574402\n",
      "32-30-train_loss===>>0.023189999163150787\n",
      "33-0-train_loss===>>0.029232075437903404\n",
      "33-10-train_loss===>>0.024405788630247116\n",
      "33-20-train_loss===>>0.024370213970541954\n",
      "33-30-train_loss===>>0.023395072668790817\n",
      "34-0-train_loss===>>0.054783910512924194\n",
      "34-10-train_loss===>>0.033046573400497437\n",
      "34-20-train_loss===>>0.022955745458602905\n",
      "34-30-train_loss===>>0.028732072561979294\n",
      "35-0-train_loss===>>0.025263849645853043\n",
      "35-10-train_loss===>>0.03356356918811798\n",
      "35-20-train_loss===>>0.049394361674785614\n",
      "35-30-train_loss===>>0.025482166558504105\n",
      "36-0-train_loss===>>0.0262434221804142\n",
      "36-10-train_loss===>>0.024826057255268097\n",
      "36-20-train_loss===>>0.02115119993686676\n",
      "36-30-train_loss===>>0.0224580317735672\n",
      "37-0-train_loss===>>0.02733435295522213\n",
      "37-10-train_loss===>>0.023778622969985008\n",
      "37-20-train_loss===>>0.025420870631933212\n",
      "37-30-train_loss===>>0.029004987329244614\n",
      "38-0-train_loss===>>0.02264328859746456\n",
      "38-10-train_loss===>>0.029607361182570457\n",
      "38-20-train_loss===>>0.024045199155807495\n",
      "38-30-train_loss===>>0.0927005335688591\n",
      "39-0-train_loss===>>0.028994306921958923\n",
      "39-10-train_loss===>>0.02566315233707428\n",
      "39-20-train_loss===>>0.03242696076631546\n",
      "39-30-train_loss===>>0.026830386370420456\n",
      "40-0-train_loss===>>0.03637773543596268\n",
      "40-10-train_loss===>>0.02596372924745083\n",
      "40-20-train_loss===>>0.03417003154754639\n",
      "40-30-train_loss===>>0.031145168468356133\n",
      "41-0-train_loss===>>0.027486568316817284\n",
      "41-10-train_loss===>>0.03354164585471153\n",
      "41-20-train_loss===>>0.02299330197274685\n",
      "41-30-train_loss===>>0.031086847186088562\n",
      "42-0-train_loss===>>0.037754371762275696\n",
      "42-10-train_loss===>>0.0250735841691494\n",
      "42-20-train_loss===>>0.02563038095831871\n",
      "42-30-train_loss===>>0.028914442285895348\n",
      "43-0-train_loss===>>0.029272984713315964\n",
      "43-10-train_loss===>>0.022680874913930893\n",
      "43-20-train_loss===>>0.028146401047706604\n",
      "43-30-train_loss===>>0.027070773765444756\n",
      "44-0-train_loss===>>0.028451917693018913\n",
      "44-10-train_loss===>>0.030068811029195786\n",
      "44-20-train_loss===>>0.02666715905070305\n",
      "44-30-train_loss===>>0.021287983283400536\n",
      "45-0-train_loss===>>0.0232587531208992\n",
      "45-10-train_loss===>>0.022627320140600204\n",
      "45-20-train_loss===>>0.026359006762504578\n",
      "45-30-train_loss===>>0.03043646737933159\n",
      "46-0-train_loss===>>0.029061278328299522\n",
      "46-10-train_loss===>>0.024382565170526505\n",
      "46-20-train_loss===>>0.023603325709700584\n",
      "46-30-train_loss===>>0.024961601942777634\n",
      "47-0-train_loss===>>0.03234698250889778\n",
      "47-10-train_loss===>>0.028920432552695274\n",
      "47-20-train_loss===>>0.022856280207633972\n",
      "47-30-train_loss===>>0.022247519344091415\n",
      "48-0-train_loss===>>0.02313033863902092\n",
      "48-10-train_loss===>>0.02651585265994072\n",
      "48-20-train_loss===>>0.03519013524055481\n",
      "48-30-train_loss===>>0.029664233326911926\n",
      "49-0-train_loss===>>0.028998425230383873\n",
      "49-10-train_loss===>>0.0237654447555542\n",
      "49-20-train_loss===>>0.03731328248977661\n",
      "49-30-train_loss===>>0.028006790205836296\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "weight_path='params/unet.pth'\n",
    "data_path=r'data'\n",
    "save_path='train_image'\n",
    "\n",
    "data_loader=DataLoader(dataset,batch_size=8,shuffle=True)\n",
    "net=UNet().to(device)\n",
    "if os.path.exists(weight_path):\n",
    "    net.load_state_dict(torch.load(weight_path))\n",
    "    print('successful load weight！')\n",
    "else:\n",
    "    print('not successful load weight')\n",
    "\n",
    "opt=optim.Adam(net.parameters())\n",
    "loss_fun=nn.BCELoss()\n",
    "epoch=50\n",
    "lossArr =[]\n",
    "for j in range(epoch):\n",
    "    lossItem = 0\n",
    "    for i,(image,segment_image) in enumerate(data_loader):\n",
    "            image, segment_image=image.to(device),segment_image.to(device)\n",
    "\n",
    "            out_image=net(image)\n",
    "            train_loss=loss_fun(out_image,segment_image)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            train_loss.backward()\n",
    "            opt.step()\n",
    "            lossItem = lossItem + train_loss.item()\n",
    "            if i%10==0:\n",
    "                print(f'{j}-{i}-train_loss===>>{train_loss.item()}')\n",
    "            \n",
    "            if i%50==0:\n",
    "                torch.save(net.state_dict(),weight_path)\n",
    "            \n",
    "            # 组合图片\n",
    "            _image=image[0]\n",
    "            _segment_image=segment_image[0]\n",
    "            _out_image=out_image[0]\n",
    "\n",
    "            img=torch.stack([_image,_segment_image,_out_image],dim=0)\n",
    "            save_image(img,f'{save_path}/{i}.png')\n",
    "    lossArr.append(lossItem)\n",
    "x =range(epoch)\n",
    "plt.figure()\n",
    "plt.plot(x, lossArr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully\n",
      "024_N_30.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m img\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_folder_path,file))\n\u001b[1;32m     18\u001b[0m img_data\u001b[38;5;241m=\u001b[39mtransform(img)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m---> 20\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m save_image(img_data[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# cv2.imshow('out',out*255.0)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# cv2.waitKey(0)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 67\u001b[0m     R1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     R2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md1(R1))\n\u001b[1;32m     69\u001b[0m     R3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc3(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md2(R2))\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mConv_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:416\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 416\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 4D input (got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "net=UNet().to('cuda')\n",
    "weights='params/unet.pth'\n",
    "if os.path.exists(weights):\n",
    "    net.load_state_dict(torch.load(weights))\n",
    "    print('successfully')\n",
    "else:\n",
    "    print('no loading')\n",
    "net.eval()\n",
    "\n",
    "\n",
    "test_file = os.listdir(test_folder_path)\n",
    "file = test_file[0]\n",
    "print(file)\n",
    "\n",
    "\n",
    "\n",
    "img=cv2.imread(os.path.join(test_folder_path,file))\n",
    "img_data=transform(img).cuda()\n",
    "    \n",
    "out=net(img_data)\n",
    "save_image(img_data[0],f'{result}/{i}.png')\n",
    "# cv2.imshow('out',out*255.0)\n",
    "# cv2.waitKey(0)\n",
    "'''\n",
    "for file,index in test_files:\n",
    "    img=cv.imread(file)\n",
    "    img_data=transform(img).cuda()\n",
    "    img_data=torch.unsqueeze(img_data,dim=0)\n",
    "    \n",
    "    out=net(img_data)\n",
    "    out=torch.argmax(out,dim=1)\n",
    "    out=torch.squeeze(out,dim=0)\n",
    "    out=out.unsqueeze(dim=0)\n",
    "    print(set((out).reshape(-1).tolist()))\n",
    "    out=(out).permute((1,2,0)).cpu().detach().numpy()\n",
    "    cv2.imwrite(f'result/result{index}.png',out)\n",
    "    cv2.imshow('out',out*255.0)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
